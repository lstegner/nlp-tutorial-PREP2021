{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc199d15-5694-4a1a-89ff-8bba729b1717",
   "metadata": {},
   "source": [
    "# Data Science Workshop: Introduction to Natural Language Processing\n",
    "### *Presented by Laura Stegner, stegner@wisc.edu*\n",
    "#### Tutorial materials adapted from [this article](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbd05b-023d-4207-9b68-3ea97a84d4d2",
   "metadata": {},
   "source": [
    "The objectives for today are as follows:\n",
    " * Discuss what NLP is, what it is used for, and some ethical considerations surrounding it\n",
    " * Learn the basics of a popular NLP Python package, `nltk`\n",
    " * Use `nltk` to perform sentiment analysis on a dataset of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac52dd1-1bff-48b0-9940-7a7834812b91",
   "metadata": {},
   "source": [
    "## Quick Review\n",
    "\n",
    "### What is NLP?\n",
    "Natural Language Processing (NLP) can be broadly thought of as the computational tools used to help computers understand and manipulate spoken or written natural language to do useful things. This goal can be achieved with the help of various NLP tasks, such as:\n",
    " * Part of speech tagging\n",
    " * Speech recognition\n",
    " * Word sense disambiguation\n",
    " * Sentiment analysis\n",
    " * Natural langauge generation\n",
    " * Named entity recognition\n",
    " * Co-reference resolution\n",
    "\n",
    "Each of the above tasks is briefly described in [this article](http://www.ibm.com/cloud/learn/natural-language-processing) by IBM. \n",
    "\n",
    "Practically, NLP is present in our everyday lives. Some common examples include autocorrect, autocomplete, *related search* terms in a web engine, email filtering, smart agents (e.g. Siri or Alexa), and machine translation (e.g. Google Translate). It is also useful in business applications such as to analyze reviews or to create automated calling systems and chat bot assistants.\n",
    "\n",
    "*Did you have any questions about this basic understanding of NLP or high-level questions about the different NLP tasks?*\n",
    "\n",
    "### What will we use NLP for?\n",
    "While NLP is being readily implemented in everyday products, it is also greatly useful in data science. NLP can be used to convert messy, unstructured natural language responses (such as interview data or open responses to survey questions) into more structured, processable data forms. Using NLP techniques to analyze data can serve to speed up processing time and also eliminate inconsistencies from manual analysis.\n",
    "\n",
    "### Where does data for NLP come from?\n",
    "Business will often use reviews or survey responses to extract information about how their business is doing. Recorded phone calls with customer service agents are also used for NLP training purposes--that's the \"this call may be recorded for quality assurance purposes\" messsage on many customer service calls! It is also common to get data from social media, such as through the [Twitter API](https://developer.twitter.com/en/docs) or [Reddit](https://www.reddit.com/wiki/api).\n",
    "\n",
    "Researchers may use either datasets attained from the web, including social media or messaging logs, or from their own work, such as transcribed interviews, open-ended survey responses, or other data collection that results in natural language.\n",
    "\n",
    "## Sentiment Analysis\n",
    "Pop quiz: *In your own words, what is sentiment analysis?*\n",
    "\n",
    "### How do we perform sentiment analysis?\n",
    "Sentiment analysis is ultimately performed with a machine learning classifier. Today, we will be using a supervised learning technique called Naive Bayes. You can watch [this video](https://www.youtube.com/watch?v=O2L2Uv9pdDA) for an introduction to it if you are curious. The algorithm itself is already provided by the `nltk` library we will be using. In fact, most of the work needs to be done to preprocess the text into the proper format; the actual classifier is simple to build and use with `nltk`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e128f-2aab-40a6-815e-33a6a2296174",
   "metadata": {},
   "source": [
    "Since we are not going to go into the details of Naive Bayes, all you need to know is that this classifier will use a labeled dataset to build a model that can classify an unknown datapoint. In our case, the data will be the tweet text, and the categories will be 'positive' or 'negative'. We will use examples of positive and negative tweets to train our model, then use that model to classify unknown tweets from a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e1419-8b65-4810-ba93-cb4bb8dd1120",
   "metadata": {},
   "source": [
    "## Getting started with Semantic Analysis\n",
    "### Setting up NLTK\n",
    "`nltk` is a great library for getting started with NLP. However, before we can use it properly, we will need to download some additional resources (which we will discuss later). To do so, we can run the following python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8eb0eb0-b82f-451f-9b5f-f56b384343c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/laura/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/laura/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/laura/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/laura/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/laura/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c69d3b-4a55-4dd2-b4fc-9e0b764b1996",
   "metadata": {},
   "source": [
    "### Getting to know the dataset\n",
    "We will be working with a dataset of tweets that are included in the `nltk` library. The library was downloaded in the setup script we just ran. We have downloaded lists of negative tweets, positive tweets, and unlabeled tweets. We will use the positive and negative tweets to train our model, and the unlabeled tweets to see how it performs!\n",
    "\n",
    "To access these different lists of tweets, we can import them individually and save them as variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b01fcea1-306c-44f0-8f8d-fe289b675132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "unlabeled_tweets = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8b94d-a366-4341-9024-571833a33cee",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "Real world natural langauge data is messing; we cannot just immediately start running a script to analyze the text. First, we must preprocess the data into a format that we can more accurately analyze. This preprocessing will involve three steps, which we will walk through below: tokenization, normalization, and noise removal.\n",
    "\n",
    "--> You can check out an example of why preprocessing is necessary in this [article](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79).\n",
    "\n",
    "The preprocessing steps from raw data into properly formatted data will follow the steps outlined below.\n",
    "#### 1. **Tokenization**\n",
    "Here, we want to break apart each review into smaller parts called *tokens*. In this case, we will break each review into tokens based on white spaces. We can tokenize datasets using the `.tokenized()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68d9d3a-a5f9-434c-8266-2a00ac0895df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "--------------------\n",
      "Tokenized tweet:\n",
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "print(\"Original tweet:\")\n",
    "print(positive_tweets[0])\n",
    "print(\"--------------------\")\n",
    "print(\"Tokenized tweet:\")\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb45a8-1f55-44d2-9fc1-d1847c36f9d3",
   "metadata": {},
   "source": [
    "#### 2. **Normalization**\n",
    "Here, we will convert words to their canonical forms. In other words, we will group words that have the same meaning but are of different form, such as \"search\" and \"searching\" and \"searched.\" To perform the normalization, we will use a technique called *lemmatization*. The lemmatization algorithm analyzes the structure of the word and its context to convert it to a normalized form.\n",
    " \n",
    "Our lemmatization process involves two steps: \n",
    " \n",
    "First, we use a part-of-speech tagger to help determine the context:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251f40d6-31c2-445b-9189-ac3416430ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "print(pos_tag(sample_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7744d-d84e-486e-aca4-d92867c4df7d",
   "metadata": {},
   "source": [
    "Second, we can use a lemmatization function. In order to correctly lemmatize words, we need to also include the part-of-speech as contextual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee6ff8d-6f85-4055-b0ff-66ce05f4d43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(lemmatize_sentence(sample_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41495884-d18d-4c0b-ad77-ca369a9e894d",
   "metadata": {},
   "source": [
    "Note that our lemmatization function includes the `pos_tag()` function, so it will not be run as a standalone step in the final script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920dd48-7600-4e1c-9df3-a2198b24f708",
   "metadata": {},
   "source": [
    "#### 3. **Noise Removal**\n",
    "Here, we will remove parts of the text that do not add any meaning or information. Exactly what constitutes noise will vary from project to project. *Stop words* are the most common words of a language. One simple approach to removing noise is to filter out all of the stop words, since they are generally irrelevant.\n",
    "\n",
    "Additionally, we will use *Regular Expressions* to filter out hyperlinks, twitter handles in replies, and punctuation/special characters. A regular expression is a sequence of characters that defines a search pattern. They are often used to validate data entry that must take a certain form (such as emails or phone numbers). You can read more about regular expressions [here](https://www.geeksforgeeks.org/write-regular-expressions/).\n",
    "\n",
    "We will use the following function to remove noise from our data. Notice that the noise removal includes lemmatization, so it will not be necessary to run lemmatization as a standalone step in the final script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0aedf33-afd1-4440-bfb9-4ef932e7576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "print(remove_noise(sample_tokens, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16f695-e53b-49fa-b295-ecaf483b207f",
   "metadata": {},
   "source": [
    "### Building the Naive Bayes Classifier\n",
    "Now that the data is ready to go, we are just missing the classifier! We will be using the `NaiveBayesClassifier`.\n",
    "\n",
    "In order to properly classify our dataset, we first need to train our model. To do this, we need to provide a combined dataset that includes the positive and negative tweets. Making this combined dataset involves cleaning the positive and negative tweets, converting the cleaned tweets to the proper form, adding the labels, and combining the positive and negative tweets into one dataset.\n",
    "\n",
    "#### 1. Clean the training data\n",
    "We will use the `remove_noise()` function that we created in order to clean both lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c54619-6006-451d-a4a7-7f866025b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4bde1-3ea9-4c70-8e80-b185134210a4",
   "metadata": {},
   "source": [
    "As a quick check, we can compare some raw tweets with some cleaned tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1cc0e1-5a26-4f36-8dba-418653a5e1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0a444-94b0-484b-a82f-8f381fadd35b",
   "metadata": {},
   "source": [
    "#### 2. Convert to dictionary\n",
    "Because of the way the `NaiveBayesClassifier` from `nltk` works, we need to transform our data from a list of words into a dictionary where each word is set to *true*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4177731f-4328-4737-b878-cab2412bd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a2fe73-818f-4417-9168-81eaac52abc5",
   "metadata": {},
   "source": [
    "#### 3. Add the labels\n",
    "Now that we properly formatted the data, we need to add the labels 'positive' and 'negative' as appropriate. Then, we can combine the positive and negative tweets into one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5603e24-ff0d-4743-9128-ec3a484d3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfe213-c2ec-4138-a012-69d9f9553ee4",
   "metadata": {},
   "source": [
    "#### 4. Combining positive and negative tweets\n",
    "The labeled positive and negative datasets need to be combined into one dataset. This is a simple step. Notice that we randomize the order of the tweets to avoid any bias from the original ordering of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d30b3fde-06e9-46c8-958a-e9e7036e2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = positive_dataset + negative_dataset\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7b1b7-0a4e-4a93-92fc-a2dc01eaa26d",
   "metadata": {},
   "source": [
    "#### 5. Split into training and testing sets\n",
    "Now that we have our training dataset, we are finally ready to build the classifier. For our Bayes Classifier to work, we need to partition the dataset into two sections: training and testing. The training data will be used to build the model, while the testing data will be used to assess the correctness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ef54a6f-40d0-41f5-96e7-f4499256a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd259c76-e157-4a49-95c1-1ad9a8c012e3",
   "metadata": {},
   "source": [
    "#### 6. Building the model\n",
    "`nltk` makes is very simple to build a Bayes Classifier now that we have jumped through all of the data preprocessing hoops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3610d27d-d41c-4a16-bd3b-0eb5c98d4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7835f9-8d25-4c31-9c0a-4d4e27b03fb2",
   "metadata": {},
   "source": [
    "#### 7. Testing the model\n",
    "Now that we have our model built, let's use the testing data to check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb0377b7-3841-4dad-8b31-374421fe1862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.996\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913eb56-40a9-4bc3-8b77-7cf64fea4006",
   "metadata": {},
   "source": [
    "#### (fun, but optional) Check out the most significant features\n",
    "Our model will identify certain words that make a tweet more likely to be positive or negative. To check out what words (*features*) it decided were most important in the classifier, we can print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb0a0790-cff9-4959-9ddc-610206be4a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2070.9 : 1.0\n",
      "                      :) = True           Positi : Negati =   1653.4 : 1.0\n",
      "                     bam = True           Positi : Negati =     23.1 : 1.0\n",
      "                followed = True           Negati : Positi =     22.9 : 1.0\n",
      "                     sad = True           Negati : Positi =     20.1 : 1.0\n",
      "                     x15 = True           Negati : Positi =     19.0 : 1.0\n",
      "                follower = True           Positi : Negati =     18.6 : 1.0\n",
      "                     via = True           Positi : Negati =     16.5 : 1.0\n",
      "                     ugh = True           Negati : Positi =     14.3 : 1.0\n",
      "                      aw = True           Negati : Positi =     13.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527ef95-0809-4fd5-8e23-d21fbbdba44f",
   "metadata": {},
   "source": [
    "### Using the Model\n",
    "Now that we have our model built, it is time to see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f9b267b-22fe-435e-aa11-f90511dac2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e65d1-b8d7-4ea0-a5f2-d8ed66fee637",
   "metadata": {},
   "source": [
    "### The Full Script\n",
    "I have included the full Python script separately from the Jupyter notebook to make it easier to see the steps without all of the commentary. You can run it from the command line: `python3 semantic_analysis.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0383a-3b80-4e61-bba3-cfceb73b8bb9",
   "metadata": {},
   "source": [
    "## Discussion/Questions\n",
    " * Questions about what we covered today?\n",
    " * What examples of NLP have you encountered either in your research or in your daily life?\n",
    " * Where do you think you could use NLP in your research, or other research situations?\n",
    " * Based on the readings or your own experiences, what are some important ethical considerations surrounding NLP?\n",
    " * Why should we care about this, and how can it affect our data analysis?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
